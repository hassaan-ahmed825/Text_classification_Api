# -*- coding: utf-8 -*-
"""classification_pytorch_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OcU3Xg46q4ZZ_KoHz5TG3JFe-GOjaU1c
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import joblib

df = pd.read_csv('Reviews.csv')

df.head()

df.columns

df.isnull().sum()

# Drop ID column
df.drop(['Id'], axis=1, inplace=True)

# Optional: Fill missing ProfileName with "Unknown"
#df['ProfileName'] = df['ProfileName'].fillna('Unknown')
# Drop rows with missing ProfileName or Summary
df = df.dropna(subset=['ProfileName', 'Summary'])

print(df.isnull().sum())

# Keep only Text and Score
df = df[['Text', 'Score']]

df.head()

# #Check class balance
# For classification:
print(df['Score'].value_counts())

# Map Score 4,5 → Positive; 1,2 → Negative; 3 → Neutral or drop
def map_sentiment(score):
    if score >= 4:
        return 'Positive'
    elif score <= 2:
        return 'Negative'
    else:
        return 'Neutral'

df['Sentiment'] = df['Score'].apply(map_sentiment)

# Optional: Drop neutral reviews if you want binary
df = df[df['Sentiment'] != 'Neutral']

print(df['Sentiment'].value_counts())

from sklearn.preprocessing import LabelEncoder

# Encode: Positive = 1, Negative = 0
le = LabelEncoder()
df['Sentiment_Label'] = le.fit_transform(df['Sentiment'])

print(df[['Sentiment', 'Sentiment_Label']].head())

# import joblib
# joblib.dump(le, 'label_encoder.pkl')

from sklearn.model_selection import train_test_split

X = df['Text']
y = df['Sentiment_Label']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

print(X_train.shape, X_test.shape)

# # Import the basic English tokenizer from torchtext
# from torchtext.data.utils import get_tokenizer

# # Import the function to build a vocabulary from an iterator
# from torchtext.vocab import build_vocab_from_iterator

# # Create a basic English tokenizer (splits text into words, lowercases, removes punctuation)
# tokenizer = get_tokenizer('basic_english')

# # Define a generator function that yields tokenized words from each text in the dataset
# def yield_tokens(data_iter):
#     for text in data_iter:
#         yield tokenizer(text)  # Tokenize each text and yield the list of tokens

# # Build the vocabulary from the tokenized texts in X_train
# # 'specials' defines special tokens like <unk> for unknown words
# vocab = build_vocab_from_iterator(
#     yield_tokens(X_train),
#     specials=["<unk>"]
# )

# # Set the default index for out-of-vocabulary words to the index of <unk>
# vocab.set_default_index(vocab["<unk>"])

# # Print the total number of unique tokens in the vocabulary, including special tokens
# print(len(vocab))

!pip uninstall -y torch torchtext torchaudio

!pip install torch==2.2.2+cpu torchtext==0.17.2+cpu --index-url https://download.pytorch.org/whl/cpu

!pip uninstall -y torchtext
!pip install torchtext==0.17.2+cpu --index-url https://download.pytorch.org/whl/cpu

from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator

tokenizer = get_tokenizer('basic_english')

def yield_tokens(data_iter):
    for text in data_iter:
        yield tokenizer(text)

vocab = build_vocab_from_iterator(yield_tokens(X_train), specials=["<unk>"])
vocab.set_default_index(vocab["<unk>"])

print(len(vocab))

import torch

def text_pipeline(x):
    return vocab(tokenizer(x))

def collate_batch(batch):
    label_list, text_list, offsets = [], [], [0]
    for (_text, _label) in batch:
        label_list.append(_label)
        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)
        text_list.append(processed_text)
        offsets.append(processed_text.size(0))
    label_list = torch.tensor(label_list, dtype=torch.int64)
    text_list = torch.cat(text_list)
    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)
    return text_list, offsets, label_list

from torch.utils.data import DataLoader

train_dataset = list(zip(X_train.tolist(), y_train.tolist()))
test_dataset = list(zip(X_test.tolist(), y_test.tolist()))

train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_batch)
test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_batch)

import torch.nn as nn

class TextClassificationModel(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_class):
        super(TextClassificationModel, self).__init__()
        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)
        self.fc = nn.Linear(embed_dim, num_class)

    def forward(self, text, offsets):
        embedded = self.embedding(text, offsets)
        return self.fc(embedded)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = TextClassificationModel(len(vocab), embed_dim=32, num_class=2).to(device)

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=4.0)

for epoch in range(5):
    model.train()
    total_acc, total_count = 0, 0
    for idx, (text, offsets, cls) in enumerate(train_dataloader):
        optimizer.zero_grad()
        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)
        output = model(text, offsets)
        loss = criterion(output, cls)
        loss.backward()
        optimizer.step()
        total_acc += (output.argmax(1) == cls).sum().item()
        total_count += cls.size(0)
    print(f'Epoch {epoch+1}, Accuracy: {total_acc/total_count:.3f}')

torch.save(model.state_dict(), 'text_classification.pt')

import pickle
with open('vocab.pkl', 'wb') as f:
    pickle.dump(vocab, f)

from google.colab import files
files.download('text_classification.pt')

